{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Volunteer Performance Dataset Generator & Analyzer\n",
        "\n",
        "This notebook provides an interactive interface for generating and analyzing volunteer performance datasets. It combines the functionality of both the dataset generator and analyzer tools into a single, easy-to-use interface.\n",
        "\n",
        "## Features\n",
        "- **Dataset Generation**: Create realistic volunteer datasets with customizable parameters\n",
        "- **Statistical Analysis**: Analyze performance metrics and correlations\n",
        "- **Promotion Criteria**: Get data-driven suggestions for volunteer promotion criteria\n",
        "- **Interactive Visualizations**: Explore data with charts and graphs\n",
        "- **Filtering Options**: Focus analysis on specific score ranges or performance levels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set up plotting style\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Import our custom classes\n",
        "from volunteer_dataset_generator import VolunteerDatasetGenerator, Task, Volunteer\n",
        "from volunteer_analyzer import VolunteerAnalyzer\n",
        "\n",
        "print(\"‚úÖ All libraries imported successfully!\")\n",
        "print(\"üìä Ready to generate and analyze volunteer datasets\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Dataset Generation\n",
        "\n",
        "### Configure Dataset Parameters\n",
        "\n",
        "Customize your dataset generation parameters below. The default values match the user's request.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset Configuration (you may change these values)\n",
        "DATASET_SIZE = 734\n",
        "HIGH_PERFORMERS = 0.45  # 45% high performers\n",
        "LOW_PERFORMERS = 0.35   # 35% low performers\n",
        "AVERAGE_PERFORMERS = 1.0 - HIGH_PERFORMERS - LOW_PERFORMERS  # 20% average performers\n",
        "MIN_TASKS = 2\n",
        "MAX_TASKS = 20\n",
        "RANDOM_SEED = 69  # For reproducible results\n",
        "OUTPUT_FILENAME = 'volunteers.csv'\n",
        "\n",
        "print(f\"üìã Dataset Configuration:\")\n",
        "print(f\"   Size: {DATASET_SIZE} volunteers\")\n",
        "print(f\"   High Performers: {HIGH_PERFORMERS*100:.0f}%\")\n",
        "print(f\"   Average Performers: {AVERAGE_PERFORMERS*100:.0f}%\")\n",
        "print(f\"   Low Performers: {LOW_PERFORMERS*100:.0f}%\")\n",
        "print(f\"   Tasks per volunteer: {MIN_TASKS}-{MAX_TASKS}\")\n",
        "print(f\"   Output file: {OUTPUT_FILENAME}\")\n",
        "print(f\"   Random seed: {RANDOM_SEED}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generate the Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the generator\n",
        "generator = VolunteerDatasetGenerator(seed=RANDOM_SEED)\n",
        "\n",
        "# Define the performance distribution\n",
        "distribution = {\n",
        "    'high_performer': HIGH_PERFORMERS,\n",
        "    'average_performer': AVERAGE_PERFORMERS,\n",
        "    'low_performer': LOW_PERFORMERS\n",
        "}\n",
        "\n",
        "print(\"üîÑ Generating dataset...\")\n",
        "\n",
        "# Generate the dataset\n",
        "volunteers = generator.generate_dataset(\n",
        "    dataset_size=DATASET_SIZE,\n",
        "    distribution=distribution,\n",
        "    task_range=(MIN_TASKS, MAX_TASKS)\n",
        ")\n",
        "\n",
        "# Save to CSV\n",
        "generator.save_to_csv(volunteers, OUTPUT_FILENAME)\n",
        "\n",
        "print(f\"‚úÖ Dataset generated successfully!\")\n",
        "print(f\"üìÅ Saved to: {OUTPUT_FILENAME}\")\n",
        "print(f\"üë• Total volunteers: {len(volunteers)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset Statistics Overview\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print basic statistics\n",
        "generator.print_statistics(volunteers)\n",
        "\n",
        "# Create a DataFrame for easier manipulation\n",
        "df = pd.DataFrame([\n",
        "    {\n",
        "        'Name': v.name,\n",
        "        'Total_Score': v.total_score,\n",
        "        'Tasks_Completed': v.tasks_completed,\n",
        "        'Average_Mark': v.average_mark,\n",
        "        'Average_Rating': v.average_rating\n",
        "    } for v in volunteers\n",
        "])\n",
        "\n",
        "print(\"\\nüìä Dataset Preview:\")\n",
        "display(df.head(10))\n",
        "\n",
        "print(\"\\nüìà Quick Statistics:\")\n",
        "display(df.describe())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualize Dataset Distribution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comprehensive visualizations\n",
        "fig = make_subplots(\n",
        "    rows=2, cols=2,\n",
        "    subplot_titles=('Total Score Distribution', 'Tasks Completed Distribution', \n",
        "                   'Average Mark Distribution', 'Average Rating Distribution'),\n",
        "    specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
        "           [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
        ")\n",
        "\n",
        "# Total Score histogram\n",
        "fig.add_trace(\n",
        "    go.Histogram(x=df['Total_Score'], name='Total Score', nbinsx=30, \n",
        "                marker_color='lightblue', opacity=0.7),\n",
        "    row=1, col=1\n",
        ")\n",
        "\n",
        "# Tasks Completed histogram\n",
        "fig.add_trace(\n",
        "    go.Histogram(x=df['Tasks_Completed'], name='Tasks Completed', nbinsx=20,\n",
        "                marker_color='lightgreen', opacity=0.7),\n",
        "    row=1, col=2\n",
        ")\n",
        "\n",
        "# Average Mark histogram\n",
        "fig.add_trace(\n",
        "    go.Histogram(x=df['Average_Mark'], name='Average Mark', nbinsx=25,\n",
        "                marker_color='lightcoral', opacity=0.7),\n",
        "    row=2, col=1\n",
        ")\n",
        "\n",
        "# Average Rating histogram\n",
        "fig.add_trace(\n",
        "    go.Histogram(x=df['Average_Rating'], name='Average Rating', nbinsx=25,\n",
        "                marker_color='lightyellow', opacity=0.7),\n",
        "    row=2, col=2\n",
        ")\n",
        "\n",
        "fig.update_layout(height=600, showlegend=False, \n",
        "                 title_text=\"Dataset Distribution Overview\")\n",
        "fig.show()\n",
        "\n",
        "print(\"üìä Dataset generation complete! Moving to analysis...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Dataset Analysis\n",
        "\n",
        "### Load and Initialize Analyzer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the analyzer with the generated dataset\n",
        "print(\"üîç Initializing analyzer...\")\n",
        "analyzer = VolunteerAnalyzer(\n",
        "    csv_file=OUTPUT_FILENAME,\n",
        "    score_filter='above',\n",
        "    min_score=0 # by default, all volunteers are included; this ensures we only include volunteers with a score above 0\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Analyzer loaded successfully!\")\n",
        "print(f\"üìä Dataset: {len(analyzer.df)} volunteers loaded for analysis\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Basic Statistical Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run basic statistics\n",
        "analyzer.basic_statistics()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Percentile Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze performance percentiles\n",
        "analyzer.percentile_analysis()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Score-Only Promotion Criteria\n",
        "\n",
        "The analyzer now uses a simplified, score-only approach for promotion criteria. This makes decisions clearer and easier to implement.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get promotion criteria suggestions (default: top 10%, percentile method)\n",
        "analyzer.suggest_promotion_criteria()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Flexible Promotion Threshold Analysis\n",
        "\n",
        "Test different percentile targets to find the right promotion threshold for your organization!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test different percentile targets with simplified score-only analysis\n",
        "print(\"üéØ TESTING DIFFERENT PERCENTILE TARGETS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Test top 5% (very selective)\n",
        "print(\"\\n1Ô∏è‚É£ TOP 5% PROMOTION CRITERIA:\")\n",
        "analyzer.show_promotion_thresholds(target_percentile=5.0)\n",
        "\n",
        "# Test top 15% (moderate)\n",
        "print(\"\\n2Ô∏è‚É£ TOP 15% PROMOTION CRITERIA:\")\n",
        "analyzer.show_promotion_thresholds(target_percentile=15.0)\n",
        "\n",
        "# Test top 25% (more inclusive)\n",
        "print(\"\\n3Ô∏è‚É£ TOP 25% PROMOTION CRITERIA:\")\n",
        "analyzer.show_promotion_thresholds(target_percentile=25.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Threshold Comparison Analysis\n",
        "\n",
        "Compare your chosen threshold with common industry standards:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare your chosen percentile with common thresholds\n",
        "print(\"üîç COMPARING DIFFERENT PROMOTION THRESHOLDS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Compare 15% with common percentiles and median/mean thresholds\n",
        "print(\"Comparing top 15% with other common promotion thresholds:\")\n",
        "analyzer.compare_promotion_percentiles(target_percentile=15.0, common_percentiles=[5, 10, 25, 50])\n",
        "\n",
        "print(\"\\nüìä Key Insights from Method Comparison:\")\n",
        "print(\"   ‚Ä¢ Percentile method: Uses relative ranking (most common approach)\")\n",
        "print(\"   ‚Ä¢ Median method: Uses middle values as benchmarks (stable, outlier-resistant)\")\n",
        "print(\"   ‚Ä¢ Mean method: Uses average values (can be affected by extreme performers)\")\n",
        "print(\"   ‚Ä¢ Choose based on your organization's philosophy and data distribution\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test Specific Method + Percentile Combinations\n",
        "\n",
        "Try different combinations to find what works best for your organization:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test specific score thresholds and percentile combinations\n",
        "print(\"üß™ TESTING SPECIFIC SCORE THRESHOLDS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Example 1: Test top 20% promotion threshold\n",
        "print(\"\\n1Ô∏è‚É£ TOP 20% PROMOTION ANALYSIS:\")\n",
        "analyzer.show_promotion_thresholds(target_percentile=20.0)\n",
        "\n",
        "# Example 2: Test top 10% promotion threshold  \n",
        "print(\"\\n2Ô∏è‚É£ TOP 10% PROMOTION ANALYSIS:\")\n",
        "analyzer.show_promotion_thresholds(target_percentile=10.0)\n",
        "\n",
        "# Example 3: Compare 12% with common thresholds\n",
        "print(\"\\n3Ô∏è‚É£ COMPARING 12% WITH COMMON THRESHOLDS:\")\n",
        "analyzer.compare_promotion_percentiles(target_percentile=12.0, common_percentiles=[5, 10, 15, 25])\n",
        "\n",
        "print(\"\\nüí° SIMPLIFIED APPROACH BENEFITS:\")\n",
        "print(\"   üéØ Score-only criteria are easy to understand and implement\")\n",
        "print(\"   üìä Clear threshold comparisons help make informed decisions\") \n",
        "print(\"   üìà Focus on total performance rather than individual metrics\")\n",
        "print(\"   üîÑ Quick comparison with industry standards (10%, 25%, 50%)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: Interactive Analysis\n",
        "\n",
        "### Custom Criteria Testing\n",
        "\n",
        "Use the cell below to test your own custom promotion criteria:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Interactive Custom Analysis - modify these values as needed\n",
        "TARGET_PERCENTILE = 15.0  # Change this to target different percentiles (5, 10, 15, 20, 25, 50, etc.)\n",
        "\n",
        "print(\"üéõÔ∏è INTERACTIVE CUSTOM ANALYSIS\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Target: Top {TARGET_PERCENTILE}% of volunteers\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Generate score-only promotion criteria\n",
        "print(\"üìä Score-Only Promotion Analysis:\")\n",
        "analyzer.suggest_promotion_criteria(target_percentile=TARGET_PERCENTILE)\n",
        "\n",
        "# Test manual score threshold\n",
        "print(f\"\\n\" + \"=\"*60)\n",
        "print(\"MANUAL SCORE THRESHOLD TEST\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "CUSTOM_MIN_SCORE = 80  # Test a specific score threshold\n",
        "\n",
        "print(\"üß™ Testing Manual Score Threshold:\")\n",
        "print(f\"   Minimum Score: {CUSTOM_MIN_SCORE}\")\n",
        "\n",
        "analyzer.test_score_threshold(min_score=CUSTOM_MIN_SCORE, show_volunteers=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Performance Distribution Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create detailed performance visualizations\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "fig.suptitle('Volunteer Performance Analysis Dashboard', fontsize=16, y=1.02)\n",
        "\n",
        "# Total Score vs Tasks Completed scatter plot\n",
        "axes[0, 0].scatter(analyzer.df['Tasks_Completed'], analyzer.df['Total_Score'], \n",
        "                  alpha=0.6, c=analyzer.df['Average_Mark'], cmap='viridis')\n",
        "axes[0, 0].set_xlabel('Tasks Completed')\n",
        "axes[0, 0].set_ylabel('Total Score')\n",
        "axes[0, 0].set_title('Total Score vs Tasks Completed\\n(Color = Average Mark)')\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Average Mark vs Average Rating scatter plot\n",
        "scatter = axes[0, 1].scatter(analyzer.df['Average_Rating'], analyzer.df['Average_Mark'], \n",
        "                           alpha=0.6, c=analyzer.df['Total_Score'], cmap='plasma')\n",
        "axes[0, 1].set_xlabel('Average Rating')\n",
        "axes[0, 1].set_ylabel('Average Mark')\n",
        "axes[0, 1].set_title('Average Mark vs Average Rating\\n(Color = Total Score)')\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "plt.colorbar(scatter, ax=axes[0, 1], label='Total Score')\n",
        "\n",
        "# Total Score distribution with percentile lines\n",
        "axes[1, 0].hist(analyzer.df['Total_Score'], bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "# Add percentile lines\n",
        "p75 = analyzer.df['Total_Score'].quantile(0.75)\n",
        "p90 = analyzer.df['Total_Score'].quantile(0.90)\n",
        "p95 = analyzer.df['Total_Score'].quantile(0.95)\n",
        "axes[1, 0].axvline(p75, color='orange', linestyle='--', label=f'75th percentile ({p75:.0f})')\n",
        "axes[1, 0].axvline(p90, color='red', linestyle='--', label=f'90th percentile ({p90:.0f})')\n",
        "axes[1, 0].axvline(p95, color='darkred', linestyle='--', label=f'95th percentile ({p95:.0f})')\n",
        "axes[1, 0].set_xlabel('Total Score')\n",
        "axes[1, 0].set_ylabel('Frequency')\n",
        "axes[1, 0].set_title('Total Score Distribution with Percentiles')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Box plot of all metrics\n",
        "metrics_data = [\n",
        "    analyzer.df['Total_Score'] / 10,  # Scale down for comparison\n",
        "    analyzer.df['Average_Mark'],\n",
        "    analyzer.df['Average_Rating'] + 2,  # Shift up for positive values\n",
        "    analyzer.df['Tasks_Completed']\n",
        "]\n",
        "axes[1, 1].boxplot(metrics_data, labels=['Total Score\\n(√∑10)', 'Avg Mark', 'Avg Rating\\n(+2)', 'Tasks'])\n",
        "axes[1, 1].set_title('Performance Metrics Distribution')\n",
        "axes[1, 1].set_ylabel('Scaled Values')\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üìä Performance dashboard complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Top Performers Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze top performers\n",
        "top_10_percent = analyzer.df.nlargest(int(len(analyzer.df) * 0.1), 'Total_Score')\n",
        "top_25_percent = analyzer.df.nlargest(int(len(analyzer.df) * 0.25), 'Total_Score')\n",
        "\n",
        "print(\"üèÜ TOP 10% PERFORMERS ANALYSIS\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Number of volunteers: {len(top_10_percent)}\")\n",
        "print(f\"Score range: {top_10_percent['Total_Score'].min():.0f} - {top_10_percent['Total_Score'].max():.0f}\")\n",
        "print(f\"Average score: {top_10_percent['Total_Score'].mean():.1f}\")\n",
        "print(f\"Average tasks: {top_10_percent['Tasks_Completed'].mean():.1f}\")\n",
        "print(f\"Average mark: {top_10_percent['Average_Mark'].mean():.2f}\")\n",
        "print(f\"Average rating: {top_10_percent['Average_Rating'].mean():.2f}\")\n",
        "\n",
        "print(\"\\n‚≠ê TOP 25% PERFORMERS ANALYSIS\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Number of volunteers: {len(top_25_percent)}\")\n",
        "print(f\"Score range: {top_25_percent['Total_Score'].min():.0f} - {top_25_percent['Total_Score'].max():.0f}\")\n",
        "print(f\"Average score: {top_25_percent['Total_Score'].mean():.1f}\")\n",
        "print(f\"Average tasks: {top_25_percent['Tasks_Completed'].mean():.1f}\")\n",
        "print(f\"Average mark: {top_25_percent['Average_Mark'].mean():.2f}\")\n",
        "print(f\"Average rating: {top_25_percent['Average_Rating'].mean():.2f}\")\n",
        "\n",
        "print(\"\\nüéØ TOP 10 INDIVIDUAL PERFORMERS\")\n",
        "print(\"=\"*50)\n",
        "top_10_individuals = analyzer.df.nlargest(10, 'Total_Score')\n",
        "for i, (_, volunteer) in enumerate(top_10_individuals.iterrows(), 1):\n",
        "    print(f\"{i:2d}. {volunteer['Name']:<25} | Score: {volunteer['Total_Score']:3.0f} | \"\n",
        "          f\"Tasks: {volunteer['Tasks_Completed']:2.0f} | Mark: {volunteer['Average_Mark']:.2f} | \"\n",
        "          f\"Rating: {volunteer['Average_Rating']:4.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: Advanced Analysis Options\n",
        "\n",
        "### Score-Filtered Analysis\n",
        "\n",
        "Analyze only volunteers above a certain score threshold:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze high-performing volunteers only (score >= 60)\n",
        "SCORE_THRESHOLD = 60\n",
        "\n",
        "print(f\"üîç ANALYZING VOLUNTEERS WITH SCORES >= {SCORE_THRESHOLD}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Create a filtered analyzer\n",
        "filtered_analyzer = VolunteerAnalyzer(csv_file=OUTPUT_FILENAME, \n",
        "                                    score_filter='above', \n",
        "                                    min_score=SCORE_THRESHOLD)\n",
        "\n",
        "if len(filtered_analyzer.df) > 0:\n",
        "    print(\"\\nüìä Filtered Dataset Statistics:\")\n",
        "    filtered_analyzer.basic_statistics()\n",
        "    \n",
        "    print(\"\\nüéØ Promotion Criteria for High Performers (Top 15%):\")\n",
        "    filtered_analyzer.suggest_promotion_criteria(target_percentile=15.0)\n",
        "    \n",
        "    print(\"\\nüîç Testing Different Thresholds for High Performers:\")\n",
        "    filtered_analyzer.compare_promotion_percentiles(target_percentile=15.0, common_percentiles=[5, 10, 25])\n",
        "else:\n",
        "    print(f\"‚ùå No volunteers found with scores >= {SCORE_THRESHOLD}\")\n",
        "    print(\"üí° Try lowering the threshold or generating a different dataset\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Export Analysis Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a summary report\n",
        "summary_stats = {\n",
        "    'Dataset Size': len(analyzer.df),\n",
        "    'Average Total Score': analyzer.df['Total_Score'].mean(),\n",
        "    'Median Total Score': analyzer.df['Total_Score'].median(),\n",
        "    'Score Standard Deviation': analyzer.df['Total_Score'].std(),\n",
        "    'Average Tasks per Volunteer': analyzer.df['Tasks_Completed'].mean(),\n",
        "    'Average Mark': analyzer.df['Average_Mark'].mean(),\n",
        "    'Average Rating': analyzer.df['Average_Rating'].mean(),\n",
        "    '75th Percentile Score': analyzer.df['Total_Score'].quantile(0.75),\n",
        "    '90th Percentile Score': analyzer.df['Total_Score'].quantile(0.90),\n",
        "    '95th Percentile Score': analyzer.df['Total_Score'].quantile(0.95)\n",
        "}\n",
        "\n",
        "# Create summary DataFrame\n",
        "summary_df = pd.DataFrame(list(summary_stats.items()), columns=['Metric', 'Value'])\n",
        "summary_df['Value'] = summary_df['Value'].round(2)\n",
        "\n",
        "print(\"üìã ANALYSIS SUMMARY REPORT\")\n",
        "print(\"=\"*40)\n",
        "display(summary_df)\n",
        "\n",
        "# Save summary to CSV\n",
        "summary_filename = 'analysis_summary.csv'\n",
        "summary_df.to_csv(summary_filename, index=False)\n",
        "print(f\"\\nüíæ Summary saved to: {summary_filename}\")\n",
        "\n",
        "# Save top performers to CSV\n",
        "top_performers_filename = 'top_performers.csv'\n",
        "top_25_percent.to_csv(top_performers_filename, index=False)\n",
        "print(f\"üíæ Top 25% performers saved to: {top_performers_filename}\")\n",
        "\n",
        "print(\"\\n‚úÖ Analysis complete! All results have been saved.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
